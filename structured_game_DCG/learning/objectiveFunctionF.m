function [score, gradient, flag] = objectiveFunctionF(theta, ...    trueTags, trueFeatureValues, featureMatrix, k, regularization, valuePrecision, verbose)       lagrangeMultipliers = featureMatrix * theta;    % lagrangeMultipliers = lagrangeMultipliers ./ abs(lagrangeMultipliers);        [~, minimizerActions, ~, minimizerProbabilities, ~, ~, maximizerValue, flag] =...        optimizerF(lagrangeMultipliers, k, valuePrecision, verbose);    %% gradient    gradient = computeGradientF(...        trueFeatureValues, minimizerActions, minimizerProbabilities, ...        featureMatrix, theta, regularization, valuePrecision);        %% score    % negative, because the objective is to maximize score,     % but the L-BFGS we use is to minimize the value.    rawScore = -(theta' * trueFeatureValues + maximizerValue);    score = rawScore;        %% regularization     regularizationValue = 0;    if(regularization.parameter ~= 0)        if(strcmpi(regularization.method, 'L1'))            regularizationValue = regularization.parameter * norm(theta, 1);        elseif(strcmpi(regularization.method, 'L2'))            regularizationValue = regularization.parameter * (theta' * theta);        else            error('Unknown regularization method ''%s''', regularization.method);        end                % since we want to minimize the value, plus regularization to penalize the large weights        score = score + regularizationValue;    end        %%    if verbose        fprintf('\t>>>> gradient=%.6f\trawScore=%.6f\tregularization[%s]=%.6f\tscore=%.6f\n', ...            norm(gradient), rawScore, regularization.method, regularizationValue, score);    endend